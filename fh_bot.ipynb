{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c759d918-fc95-4589-8415-40ef09162b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "from config.settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700da9b0-203e-49df-bfce-2ecb3099c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f699d3f1-720e-48e6-8a9d-8024e809b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(soup: BeautifulSoup, uri: str) -> List[Dict]:\n",
    "        sections = soup.find_all(\"section\")\n",
    "        section_list = []\n",
    "        for i, section in enumerate(sections):\n",
    "            section_id = section.get(\"id\")\n",
    "            section_text = extract_text_from_section(section)\n",
    "            if section_id:\n",
    "                section_data = {\n",
    "                    \"source\": f\"{uri}#{section_id}\",\n",
    "                    \"text\": section_text,\n",
    "                    \"previous_section\": section_list[i-1]['source'] if i > 0 else None,\n",
    "                    \"next_section\": None,\n",
    "                    \"metadata\": {\n",
    "                        \"page_heading\": soup.find(\"h1\").get_text().strip() if soup.find(\"h1\") else Path(uri).stem,\n",
    "                        \"section_id\": section_id\n",
    "                    }\n",
    "                }\n",
    "                if i > 0:\n",
    "                    section_list[i-1]['next_section'] = section_data['source']\n",
    "                section_list.append(section_data)\n",
    "        return section_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8758ed1f-1526-4b4d-9584-2e56abba24ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_section(section) -> str:\n",
    "        texts = []\n",
    "        for element in section.children:\n",
    "            if isinstance(element, NavigableString):\n",
    "                if element.strip():\n",
    "                    texts.append(element.strip())\n",
    "            elif element.name != 'section':\n",
    "                texts.append(element.get_text().strip())\n",
    "        return clean_text(\" \".join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd72f2a-629d-49cb-9c2a-a33a59443bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_uri(path: Path, scheme: str = \"https://\", domain: str = \"docs.fastht.ml\") -> str:\n",
    "        relative_path = str(path.relative_to(settings.RAW_DATA_DIR)).replace(\"\\\\\", \"/\")\n",
    "        return scheme + domain + \"/\" + relative_path\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Replace multiple newlines with a single space\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "def process_html_files(html_files_path: List[Path]) -> List[Dict]:\n",
    "    docs_text = []\n",
    "    for record in html_files_path:\n",
    "        print(\"processing: \", record)\n",
    "        with open(record, \"r\", encoding=\"utf-8\") as html_file:\n",
    "            soup = BeautifulSoup(html_file, \"html.parser\")\n",
    "        uri = path_to_uri(path=record)\n",
    "        sections = extract_sections(soup, uri)\n",
    "        docs_text.append(sections)\n",
    "    return docs_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d48e92a-5949-47aa-9d48-225fe83de36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/index.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/ref/defining_xt_component.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/ref/live_reload.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/ref/handlers.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/api/oauth.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/api/core.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/api/cli.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/api/fastapp.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/api/components.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/api/xtend.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/api/js.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/api/pico.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/tutorials/tutorial_for_web_devs.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/tutorials/e2e.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/tutorials/quickstart_for_web_devs.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/tutorials/by_example.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/tutorials/index.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/explains/oauth.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/explains/faq.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/explains/explaining_xt_components.html\n",
      "processing:  /home/amrit/data-projects/llms/fasthtml-docs-bot/data/raw-data/explains/routes.html\n",
      "Total documents processed: 21\n"
     ]
    }
   ],
   "source": [
    "html_files_path = [path for path in settings.RAW_DATA_DIR.rglob(\"*.html\") if not path.is_dir()]\n",
    "docs_text = process_html_files(html_files_path)\n",
    "print(f\"Total documents processed: {len(docs_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c353758-10ea-48b6-b800-acc6e33975f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'source': 'https://docs.fastht.ml/index.html#installation',\n",
       "   'text': 'Installation Since fasthtml is a Python library, you can install it with: pip install python-fasthtml In the near future, we hope to add component libraries that can likewise be installed via pip .',\n",
       "   'previous_section': None,\n",
       "   'next_section': 'https://docs.fastht.ml/index.html#usage',\n",
       "   'metadata': {'page_heading': 'FastHTML', 'section_id': 'installation'}},\n",
       "  {'source': 'https://docs.fastht.ml/index.html#usage',\n",
       "   'text': 'Usage For a minimal app, create a file “main.py” as follows: main.py from fasthtml.common import * app,rt = fast_app() @rt(\\'/\\') def get(): return Div(P(\\'Hello World!\\'), hx_get=\"/change\") serve() Running the app with python main.py prints out a link to your running app: http://localhost:5001 . Visit that link in your browser and you should see a page with the text “Hello World!”. Congratulations, you’ve just created your first FastHTML app! Adding interactivity is surprisingly easy, thanks to HTMX. Modify the file to add this function: main.py @rt(\\'/change\\') def get(): return P(\\'Nice to be here!\\') You now have a page with a clickable element that changes the text when clicked. When clicking on this link, the server will respond with an “HTML partial”—that is, just a snippet of HTML which will be inserted into the existing page. In this case, the returned element will replace the original P element (since that’s the default behavior of HTMX) with the new version returned by the second route. This “hypermedia-based” approach to web development is a powerful way to build web applications.',\n",
       "   'previous_section': 'https://docs.fastht.ml/index.html#installation',\n",
       "   'next_section': 'https://docs.fastht.ml/index.html#getting-help-from-ai',\n",
       "   'metadata': {'page_heading': 'FastHTML', 'section_id': 'usage'}},\n",
       "  {'source': 'https://docs.fastht.ml/index.html#getting-help-from-ai',\n",
       "   'text': 'Getting help from AI Because FastHTML is newer than most LLMs, AI systems like Cursor, ChatGPT, Claude, and Copilot won’t give useful answers about it. To fix that problem, we’ve provided an LLM-friendly guide that teaches them how to use FastHTML. To use it, add this link for your AI helper to use: /llms-ctx.txt This example is in a format based on recommendations from Anthropic for use with Claude Projects . This works so well that we’ve actually found that Claude can provide even better information than our own documentation! For instance, read through this annotated Claude chat for some great getting-started information, entirely generated from a project using the above text file as context. If you use Cursor, type @doc then choose “ Add new doc ”, and use the /llms-ctx.txt link above. The context file is auto-generated from our llms.txt (our proposed standard for providing AI-friendly information)—you can generate alternative versions suitable for other models as needed.',\n",
       "   'previous_section': 'https://docs.fastht.ml/index.html#usage',\n",
       "   'next_section': 'https://docs.fastht.ml/index.html#next-steps',\n",
       "   'metadata': {'page_heading': 'FastHTML',\n",
       "    'section_id': 'getting-help-from-ai'}},\n",
       "  {'source': 'https://docs.fastht.ml/index.html#next-steps',\n",
       "   'text': 'Next Steps Start with the official sources to learn more about FastHTML: About : Learn about the core ideas behind FastHTML Documentation : Learn from examples how to write FastHTML code Idiomatic app : Heavily commented source code walking through a complete application, including custom authentication, JS library connections, and database use. We also have a 1-hour intro video: The capabilities of FastHTML are vast and growing, and not all the features and patterns have been documented yet. Be prepared to invest time into studying and modifying source code, such as the main FastHTML repo’s notebooks and the official FastHTML examples repo: FastHTML Examples Repo on GitHub FastHTML Repo on GitHub Then explore the small but growing third-party ecosystem of FastHTML tutorials, notebooks, libraries, and components: FastHTML Gallery : Learn from minimal examples of components (ie chat bubbles, click-to-edit, infinite scroll, etc) Creating Custom FastHTML Tags for Markdown Rendering by Isaac Flath Your tutorial here! Finally, join the FastHTML community to ask questions, share your work, and learn from others: Discord',\n",
       "   'previous_section': 'https://docs.fastht.ml/index.html#getting-help-from-ai',\n",
       "   'next_section': None,\n",
       "   'metadata': {'page_heading': 'FastHTML', 'section_id': 'next-steps'}}]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_text[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91fcc93-2fbc-405a-bf6b-465affd8eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc5be52f-7589-4867-9ff0-b77a2d531b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, chunk_size: int, chunk_overlap: int):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "    def chunk_section(self, section: Dict[str, str]) -> List[Dict[str, str]]:\n",
    "        chunks = self.text_splitter.create_documents(\n",
    "            texts=[section[\"text\"]],\n",
    "            metadatas=[{\n",
    "                \"source\": section[\"source\"],\n",
    "                \"previous_section\": section.get(\"previous_section\"),\n",
    "                \"next_section\": section.get(\"next_section\"),\n",
    "                \"metadata\": section.get(\"metadata\")\n",
    "            }]\n",
    "        )\n",
    "        return [{\n",
    "            \"text\": chunk.page_content,\n",
    "            \"source\": chunk.metadata[\"source\"],\n",
    "            \"previous_section\": chunk.metadata[\"previous_section\"],\n",
    "            \"next_section\": chunk.metadata[\"next_section\"],\n",
    "            \"metadata\": chunk.metadata[\"metadata\"]\n",
    "        } for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2101620-9360-4d92-8ec6-359bccbd8556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 534\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    preprocessor = Preprocessor(chunk_size=settings.CHUNK_SIZE, chunk_overlap=settings.CHUNK_OVERLAP)\n",
    "    chunked_docs_text = []\n",
    "    for doc in docs_text:\n",
    "        for section in doc:\n",
    "            chunked_sections = preprocessor.chunk_section(section)\n",
    "            chunked_docs_text.extend(chunked_sections)\n",
    "    \n",
    "    print(f\"Total chunks created: {len(chunked_docs_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800dd40f-05ec-40d0-8d69-9dd083488eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Installation Since fasthtml is a Python library, you can install it with: pip install python-fasthtml In the near future, we hope to add component libraries that can likewise be installed via pip .',\n",
       "  'source': 'https://docs.fastht.ml/index.html#installation',\n",
       "  'previous_section': None,\n",
       "  'next_section': 'https://docs.fastht.ml/index.html#usage',\n",
       "  'metadata': {'page_heading': 'FastHTML', 'section_id': 'installation'}},\n",
       " {'text': 'Usage For a minimal app, create a file “main.py” as follows: main.py from fasthtml.common import * app,rt = fast_app() @rt(\\'/\\') def get(): return Div(P(\\'Hello World!\\'), hx_get=\"/change\") serve() Running the app with python main.py prints out a link to your running app: http://localhost:5001 . Visit that link in your browser and you should see a page with the text “Hello World!”. Congratulations, you’ve just created your first FastHTML app! Adding interactivity is surprisingly easy, thanks to',\n",
       "  'source': 'https://docs.fastht.ml/index.html#usage',\n",
       "  'previous_section': 'https://docs.fastht.ml/index.html#installation',\n",
       "  'next_section': 'https://docs.fastht.ml/index.html#getting-help-from-ai',\n",
       "  'metadata': {'page_heading': 'FastHTML', 'section_id': 'usage'}},\n",
       " {'text': \"interactivity is surprisingly easy, thanks to HTMX. Modify the file to add this function: main.py @rt('/change') def get(): return P('Nice to be here!') You now have a page with a clickable element that changes the text when clicked. When clicking on this link, the server will respond with an “HTML partial”—that is, just a snippet of HTML which will be inserted into the existing page. In this case, the returned element will replace the original P element (since that’s the default behavior of\",\n",
       "  'source': 'https://docs.fastht.ml/index.html#usage',\n",
       "  'previous_section': 'https://docs.fastht.ml/index.html#installation',\n",
       "  'next_section': 'https://docs.fastht.ml/index.html#getting-help-from-ai',\n",
       "  'metadata': {'page_heading': 'FastHTML', 'section_id': 'usage'}},\n",
       " {'text': 'P element (since that’s the default behavior of HTMX) with the new version returned by the second route. This “hypermedia-based” approach to web development is a powerful way to build web applications.',\n",
       "  'source': 'https://docs.fastht.ml/index.html#usage',\n",
       "  'previous_section': 'https://docs.fastht.ml/index.html#installation',\n",
       "  'next_section': 'https://docs.fastht.ml/index.html#getting-help-from-ai',\n",
       "  'metadata': {'page_heading': 'FastHTML', 'section_id': 'usage'}},\n",
       " {'text': 'Getting help from AI Because FastHTML is newer than most LLMs, AI systems like Cursor, ChatGPT, Claude, and Copilot won’t give useful answers about it. To fix that problem, we’ve provided an LLM-friendly guide that teaches them how to use FastHTML. To use it, add this link for your AI helper to use: /llms-ctx.txt This example is in a format based on recommendations from Anthropic for use with Claude Projects . This works so well that we’ve actually found that Claude can provide even better',\n",
       "  'source': 'https://docs.fastht.ml/index.html#getting-help-from-ai',\n",
       "  'previous_section': 'https://docs.fastht.ml/index.html#usage',\n",
       "  'next_section': 'https://docs.fastht.ml/index.html#next-steps',\n",
       "  'metadata': {'page_heading': 'FastHTML',\n",
       "   'section_id': 'getting-help-from-ai'}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_docs_text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bebf23ef-1d57-460e-9c15-5eee962d7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "804349e9-ddb9-45ad-8b78-4b08fdd67f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_embedding_model: Returns an embedding model based on the specified model name.\n",
    "# OpenAIEmbeddings: Uses OpenAI’s API for embedding.\n",
    "# HuggingFaceEmbeddings: Uses Hugging Face’s models for embedding.\n",
    "\n",
    "def get_embedding_model(embedding_model_name, model_kwargs, encode_kwargs):\n",
    "    if embedding_model_name == \"text-embedding-ada-002\":\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            model = embedding_model_name,\n",
    "            openai_api_base = os.environ[\"OPENAI_API_BASE\"],\n",
    "            openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        )\n",
    "    else:\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name = embedding_model_name, # also works with model_path\n",
    "            model_kwargs = model_kwargs,\n",
    "            encode_kwargs = encode_kwargs\n",
    "        )\n",
    "    return embedding_model\n",
    "\n",
    "# EmbedChunks: A class to embed chunks using the specified model.\n",
    "# init: Initializes the embedding model.\n",
    "# call: Embeds the text in the batch and returns the embeddings along with the original text and source.\n",
    "\n",
    "class EmbedChunks:\n",
    "    def __init__(self, model_name):\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            embedding_model_name = model_name,\n",
    "            model_kwargs = {\"device\": \"cuda\"},\n",
    "            encode_kwargs = {\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        embeddings = self.embedding_model.embed_documents(batch[\"text\"])\n",
    "        return {\"text\": batch[\"text\"], \"source\": batch[\"source\"], \"embeddings\": embeddings}\n",
    "\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        )\n",
    "\n",
    "    def embed_chunks(self, chunks: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        embeddings = self.embedding_model.embed_documents(texts)\n",
    "        return [\n",
    "            {\"text\": chunk[\"text\"], \"source\": chunk[\"source\"], \"embedding\": embedding}\n",
    "            for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46727545-0b5b-4a82-bde2-aa25ccac8b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amrit/miniforge3/envs/llmdev/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "INFO:datasets:PyTorch version 2.4.1 available.\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: thenlper/gte-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings created: 534\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    embedder = Embedder(model_name=settings.EMBEDDING_MODEL_NAME)\n",
    "    embedded_chunks = embedder.embed_chunks(chunked_docs_text)\n",
    "\n",
    "    print(f\"Total embeddings created: {len(embedded_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18083687-87cb-4f2f-80cc-a694d7adc6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/storage/vector_store.py\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c96dcd5-c176-4f12-940a-9eb64b9142a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, embedding_model, persist_directory: Path):\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=\"fasthtml_docs_db\",\n",
    "            embedding_function=embedding_model,\n",
    "            persist_directory=os.path.abspath(persist_directory)\n",
    "        )\n",
    "\n",
    "    def add_documents(self, documents: List[Dict[str, str]]):\n",
    "        docs = [\n",
    "            Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "            for doc in documents\n",
    "        ]\n",
    "        self.vector_store.add_documents(docs)\n",
    "        logger.info(f\"Added {len(docs)} documents to the vector store\")\n",
    "\n",
    "    def similarity_search(self, query: str, k: int = 5) -> List[Dict[str, str]]:\n",
    "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "        return [\n",
    "            {\n",
    "                \"text\": doc.page_content,\n",
    "                \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "                \"score\": score\n",
    "            }\n",
    "            for doc, score in results\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ad96989-bc50-4480-ae4b-df40f77b2338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: thenlper/gte-base\n",
      "INFO:chromadb.telemetry.posthog:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in name='fasthtml_docs_db' id=UUID('f5836f62-38aa-40d4-add4-b2b4d063ece3') metadata=None: 534\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=settings.EMBEDDING_MODEL_NAME,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100}\n",
    "    )\n",
    "\n",
    "    # Initialize the vector store\n",
    "    vectorStore = VectorStore(embedding_model=embedding_model, persist_directory=settings.VECTOR_STORE_DIR)\n",
    "    \n",
    "    # Assuming embedded_chunks is already defined and contains the embedded documents\n",
    "    vectorStore.add_documents(embedded_chunks)\n",
    "\n",
    "    # Assuming you have a client instance to interact with the Chroma DB\n",
    "    client = vectorStore.vector_store._client\n",
    "    collection = client.get_collection(name=\"fasthtml_docs_db\")\n",
    "\n",
    "    print(f\"Total items in {collection}: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb17802-f39a-41a4-91b5-1bec626e147c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Installing FastHTML FastHTML is just Python . Installation is often done with pip: pip install python-fasthtml',\n",
       "  'source': 'https://docs.fastht.ml/tutorials/tutorial_for_web_devs.html#installing-fasthtml',\n",
       "  'score': 0.1384994089603424},\n",
       " {'text': 'Install FastHTML For Mac, Windows and Linux, enter: pip install python-fasthtml',\n",
       "  'source': 'https://docs.fastht.ml/tutorials/e2e.html#install-fasthtml',\n",
       "  'score': 0.1573334038257599},\n",
       " {'text': 'Installation pip install python-fasthtml',\n",
       "  'source': 'https://docs.fastht.ml/tutorials/quickstart_for_web_devs.html#installation',\n",
       "  'score': 0.1606634110212326},\n",
       " {'text': 'Installation Since fasthtml is a Python library, you can install it with: pip install python-fasthtml In the near future, we hope to add component libraries that can likewise be installed via pip .',\n",
       "  'source': 'https://docs.fastht.ml/index.html#installation',\n",
       "  'score': 0.1731264591217041},\n",
       " {'text': 'FastHTML Basics FastHTML is just Python . You can install it with pip install python-fasthtml . Extensions/components built for it can likewise be distributed via PyPI or as simple Python files. The core usage of FastHTML is to define routes, and then to define what to do at each route. This is similar to the FastAPI web framework (in fact we implemented much of the functionality to match the FastAPI usage examples), but where FastAPI focuses on returning JSON data to build APIs, FastHTML',\n",
       "  'source': 'https://docs.fastht.ml/tutorials/by_example.html#fasthtml-basics',\n",
       "  'score': 0.19991883635520935}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorStore.similarity_search('how to install fasthtml?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf4c43-5bbe-43c5-892a-c2779d356931",
   "metadata": {},
   "source": [
    "### Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "802a4cbd-ab30-4f70-96ac-81bfaf029859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b90827a0-35c2-40f5-9e0d-7c39cc201f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_stream(chat_completion, llm):\n",
    "    if llm.startswith(\"gpt\"):\n",
    "        for chunk in chat_completion:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content is not None:\n",
    "                yield content\n",
    "    else:\n",
    "        for chunk in chat_completion:\n",
    "            yield chunk\n",
    "\n",
    "def prepare_response(chat_completion, stream, llm):\n",
    "    if stream:\n",
    "        return response_stream(chat_completion, llm)\n",
    "    else:\n",
    "        if llm.startswith(\"gpt\"):\n",
    "            return chat_completion.choices[0].message.content\n",
    "        else:\n",
    "            return chat_completion\n",
    "\n",
    "\n",
    "def get_client(llm):\n",
    "    if llm.startswith(\"gpt\"):\n",
    "        base_url = os.environ[\"OPENAI_API_BASE\"]\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        client = openai.OpenAI(base_url=base_url, api_key=api_key)\n",
    "    else: \n",
    "        #base_url = os.environ[\"HUGGINGFACEHUB_API_BASE\"]\n",
    "        api_key = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "        client = HuggingFaceEndpoint(huggingfacehub_api_token=api_key, repo_id=llm)\n",
    "    \n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "24f01da8-b2e0-4578-b045-d8886457a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    llm, temperature=0.0, stream=True, \n",
    "    system_content=\"\", assistant_content=\"\", user_content=\"\", \n",
    "    max_retries=1, retry_interval=60):\n",
    "    \"\"\"Generate response from an LLM\"\"\"\n",
    "    retry_count = 0\n",
    "    client = get_client(llm=llm)\n",
    "    \n",
    "    prompt = [(\"system\", system_content), (\"assistant\", assistant_content), (\"user\", user_content)]\n",
    "    messages = [{\"role\": role, \"content\": content} for role, content in prompt if content]\n",
    "\n",
    "    while retry_count <= max_retries:\n",
    "        try:\n",
    "            if llm.startswith(\"gpt\"):\n",
    "                chat_completion = client.chat.completions.create(\n",
    "                    model=llm,\n",
    "                    temperature=temperature,\n",
    "                    stream=stream,\n",
    "                    messages=messages,\n",
    "                )\n",
    "            else:\n",
    "                chat_completion = client.invoke(\n",
    "                    repo_id=llm,\n",
    "                    temperature=temperature,\n",
    "                    streaming=stream,\n",
    "                    input=messages,\n",
    "                )\n",
    "            return prepare_response(chat_completion, stream=stream, llm=llm)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Exception: {e}')\n",
    "            time.sleep(retry_interval) # default is pre-minute rate limits\n",
    "            retry_count += 1\n",
    "    return \"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "efec31ad-2e81-40ec-8a47-e023638fb434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Installing FastHTML FastHTML is just Python . Installation is often done with pip: pip install python-fasthtml', 'Install FastHTML For Mac, Windows and Linux, enter: pip install python-fasthtml', 'Installation pip install python-fasthtml', 'Installation Since fasthtml is a Python library, you can install it with: pip install python-fasthtml In the near future, we hope to add component libraries that can likewise be installed via pip .', 'FastHTML Basics FastHTML is just Python . You can install it with pip install python-fasthtml . Extensions/components built for it can likewise be distributed via PyPI or as simple Python files. The core usage of FastHTML is to define routes, and then to define what to do at each route. This is similar to the FastAPI web framework (in fact we implemented much of the functionality to match the FastAPI usage examples), but where FastAPI focuses on returning JSON data to build APIs, FastHTML']\n"
     ]
    }
   ],
   "source": [
    "context_results = vectorStore.similarity_search('how to install fasthtml?')\n",
    "context = [item[\"text\"] for item in context_results]\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c765a0f-794a-4856-a4f6-d4b65db4f55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/amrit/.cache/huggingface/token\n",
      "Login successful\n",
      "\n",
      "\n",
      "To install FastHTML, use the command `pip install python-fasthtml`. This command works on Mac, Windows, and Linux systems. FastHTML is a Python library, so it can be installed using pip."
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "query = 'how to install fasthtml?'\n",
    "response = generate_response(\n",
    "    llm=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    temperature=0.1,\n",
    "    stream=True,\n",
    "    system_content=\"Answer the query using the context provided. Be succinct.\",\n",
    "    user_content=f\"query: {query}, context: {context}\"\n",
    ")\n",
    "\n",
    "# Stream response\n",
    "for content in response:\n",
    "    print(content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2b87b27-ecda-4675-ad50-7ced6eab02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def get_num_tokens(text):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "\n",
    "def trim(text, max_context_length):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return enc.decode(enc.encode(text)[:max_context_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4e86ab26-d7ed-4642-a982-fcc1b6ee828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryAgent:\n",
    "    def __init__(self, embedding_model_name=\"thenlper/gte-base\",\n",
    "                 llm=\"mistralai/Mistral-7B-Instruct-v0.3\", temperature=0.1, \n",
    "                 max_context_length=4096, system_content=\"\", assistant_content=\"\"\n",
    "                ):\n",
    "        # Embedding model\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            embedding_model_name = embedding_model_name,\n",
    "            model_kwargs = {\"device\": \"cuda\"},\n",
    "            encode_kwargs = {\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        )\n",
    "\n",
    "        # Context length (restrict input length to 50% of total context length)\n",
    "        max_context_length = int(0.5*max_context_length)\n",
    "\n",
    "        # LLM\n",
    "        self.llm = llm\n",
    "        self.temperature = temperature\n",
    "        self.context_length = max_context_length - get_num_tokens(system_content + assistant_content)\n",
    "        self.system_content = system_content\n",
    "        self.assistant_content = assistant_content\n",
    "\n",
    "    def __call__(self, query, num_chunks=5, stream=True):\n",
    "        # Get sources and context\n",
    "        # Initialize the vector store\n",
    "        vectorStore = VectorStore(embedding_model=embedding_model_name, persist_directory=settings.VECTOR_STORE_DIR)\n",
    "        context_results = vectorStore.similarity_search(query=query)\n",
    "\n",
    "        # Generate response\n",
    "        context = [item[\"text\"] for item in context_results]\n",
    "        sources = [item[\"source\"] for item in context_results]\n",
    "        user_content = f\"query: {query}, context: {context}\"\n",
    "        answer = generate_response(\n",
    "            llm=self.llm,\n",
    "            temperature=self.temperature,\n",
    "            stream=stream,\n",
    "            system_content=self.system_content,\n",
    "            assistant_content=self.assistant_content,\n",
    "            user_content=trim(user_content, self.context_length))\n",
    "\n",
    "        # Result\n",
    "        result = {\n",
    "            \"question\": query,\n",
    "            \"sources\": sources,\n",
    "            \"answer\": answer,\n",
    "            \"llm\": self.llm,\n",
    "        }\n",
    "        return result\n",
    "\n",
    "# context_results = vectorStore.similarity_search('how to install fasthtml?')\n",
    "# context = [item[\"text\"] for item in context_results]\n",
    "\n",
    "# query = 'how to install fasthtml?'\n",
    "# response = generate_response(\n",
    "#     llm=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#     temperature=0.1,\n",
    "#     stream=True,\n",
    "#     system_content=\"Answer the query using the context provided. Be succinct.\",\n",
    "#     user_content=f\"query: {query}, context: {context}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e7b338aa-5fbf-496c-9f14-3988eb854933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from config.config import MAX_CONTEXT_LENGTHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10834023-cac3-41c2-a103-5d42d1c712f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "llm = \"mistralai/Mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "95483e86-1447-4c51-88ee-2cd140efcc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: thenlper/gte-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/amrit/.cache/huggingface/token\n",
      "Login successful\n",
      "{\n",
      "  \"question\": \"how to install fasthtml?\",\n",
      "  \"sources\": [\n",
      "    \"https://docs.fastht.ml/tutorials/tutorial_for_web_devs.html#installing-fasthtml\",\n",
      "    \"https://docs.fastht.ml/tutorials/e2e.html#install-fasthtml\",\n",
      "    \"https://docs.fastht.ml/tutorials/quickstart_for_web_devs.html#installation\",\n",
      "    \"https://docs.fastht.ml/index.html#installation\",\n",
      "    \"https://docs.fastht.ml/tutorials/by_example.html#fasthtml-basics\"\n",
      "  ],\n",
      "  \"answer\": \"\\n\\nTo install FastHTML, use the command `pip install python-fasthtml`. This command works on Mac, Windows, and Linux systems. FastHTML is a Python library, so it can be installed using pip.\",\n",
      "  \"llm\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = 'how to install fasthtml?'\n",
    "system_content = \"Answer the query using the context provided. Be succinct.\"\n",
    "agent = QueryAgent(\n",
    "    embedding_model_name=embedding_model_name,\n",
    "    llm=llm,\n",
    "    max_context_length=MAX_CONTEXT_LENGTHS[llm],\n",
    "    system_content=system_content)\n",
    "result = agent(query=query, stream=False)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d556981-f20b-479a-af84-fc446ef4b799",
   "metadata": {},
   "source": [
    "I am building a Q&A bot for Fasthtml docs website so that users can ask any query related to Fasthtml. Complete FastHTML Docs Bot Project Structure and Code is included. I have included current code and structure. I have made some changes to code so, i have added new code as well. your task to include the changes to the code base. if possible, try include these best pratices in the code further optimize this for production environment.\n",
    "1. Use pydantic for configuration management and data validation.\n",
    "2. Consider using asyncio for asynchronous processing where applicable.\n",
    "3. Implement logging instead of print statements for better debugging.\n",
    "4. Use tqdm for progress bars in long-running processes.\n",
    "5. Consider using typer for creating command-line interfaces for your scripts.\n",
    "6. For large-scale processing, consider using Ray for distributed processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1797a-c27a-4b02-b049-e84850bdacec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current code and Project Structure\n",
    "\n",
    "fasthtml-docs-bot/\n",
    "│\n",
    "├── src/\n",
    "│   ├── data/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── downloader.py\n",
    "│   │   ├── extractor.py\n",
    "│   │   └── preprocessor.py\n",
    "│   │\n",
    "│   ├── embedding/\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── embedder.py\n",
    "│   │\n",
    "│   ├── storage/\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── vector_store.py\n",
    "│   │\n",
    "│   ├── query/\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── searcher.py\n",
    "│   │\n",
    "│   └── utils/\n",
    "│       ├── __init__.py\n",
    "│       └── helpers.py\n",
    "│\n",
    "├── scripts/\n",
    "│   ├── download_docs.py\n",
    "│   ├── process_docs.py\n",
    "│   ├── create_embeddings.py\n",
    "│   └── query_docs.py\n",
    "│\n",
    "├── tests/\n",
    "│   ├── test_downloader.py\n",
    "│   ├── test_extractor.py\n",
    "│   ├── test_embedder.py\n",
    "│   └── test_searcher.py\n",
    "│\n",
    "├── data/\n",
    "│   ├── raw-data/\n",
    "│   ├── processed-data/\n",
    "│   └── docs_db/\n",
    "│\n",
    "├── config/\n",
    "│   ├── __init__.py\n",
    "│   └── settings.py\n",
    "│   └── config.py\n",
    "│\n",
    "├── requirements.txt\n",
    "├── setup.py\n",
    "├── README.md\n",
    "└── .gitignore\n",
    "\n",
    "# Code for each file\n",
    "\n",
    "# config/settings.py\n",
    "\n",
    "from pydantic import BaseSettings\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    BASE_URL: str = \"https://docs.fastht.ml/\"\n",
    "    RAW_DATA_DIR: str = \"data/raw-data\"\n",
    "    PROCESSED_DATA_DIR: str = \"data/processed-data\"\n",
    "    VECTOR_STORE_DIR: str = \"data/fasthtml_docs_db\"\n",
    "    CHUNK_SIZE: int = 500\n",
    "    CHUNK_OVERLAP: int = 50\n",
    "    EMBEDDING_MODEL_NAME: str = \"thenlper/gte-base\"\n",
    "\n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "\n",
    "settings = Settings()\n",
    "\n",
    "# src/data/downloader.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from config.settings import settings\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HTMLDownloader:\n",
    "    def __init__(self, base_url: str, output_dir: str):\n",
    "        self.base_url = base_url\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.downloaded_urls = set()\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_fixed(2))\n",
    "    def fetch_url(self, url: str) -> requests.Response:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "\n",
    "    def download_page(self, url: str) -> list:\n",
    "        if url in self.downloaded_urls:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            response = self.fetch_url(url)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f'Failed to download: {url} due to {e}')\n",
    "            return []\n",
    "\n",
    "        self.downloaded_urls.add(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        file_name = url.split('/')[-1] or 'index.html'\n",
    "        file_path = self.output_dir / file_name\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(soup.prettify())\n",
    "        logger.info(f'Downloaded: {url}')\n",
    "\n",
    "        return [\n",
    "            urljoin(url, link['href'])\n",
    "            for link in soup.find_all('a', href=True)\n",
    "            if urljoin(url, link['href']).startswith(self.base_url) and \n",
    "               urljoin(url, link['href']).endswith('.html')\n",
    "        ]\n",
    "\n",
    "    def download_all(self):\n",
    "        to_download = [self.base_url]\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            with tqdm(total=len(to_download), desc=\"Downloading pages\") as pbar:\n",
    "                while to_download:\n",
    "                    futures = [executor.submit(self.download_page, url) for url in to_download]\n",
    "                    to_download = []\n",
    "                    for future in as_completed(futures):\n",
    "                        result = future.result()\n",
    "                        if result:\n",
    "                            to_download.extend(result)\n",
    "                        pbar.update(1)\n",
    "                    pbar.total = pbar.n + len(to_download)\n",
    "\n",
    "# src/data/extractor.py\n",
    "\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HTMLExtractor:\n",
    "    @staticmethod\n",
    "    def extract_sections(html_file: Path) -> List[Dict[str, str]]:\n",
    "        with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "        sections = soup.find_all(\"section\")\n",
    "        section_list = []\n",
    "        for section in sections:\n",
    "            section_id = section.get(\"id\")\n",
    "            section_text = HTMLExtractor.extract_text_from_section(section)\n",
    "            if section_id:\n",
    "                uri = HTMLExtractor.path_to_uri(path=html_file)\n",
    "                section_list.append({\"source\": f\"{uri}#{section_id}\", \"text\": section_text})\n",
    "        \n",
    "        logger.info(f\"Extracted {len(section_list)} sections from {html_file}\")\n",
    "        return section_list\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_text_from_section(section) -> str:\n",
    "        texts = []\n",
    "        for elem in section.children:\n",
    "            if isinstance(elem, NavigableString):\n",
    "                if elem.strip():\n",
    "                    texts.append(elem.strip())\n",
    "            elif elem.name == 'section':\n",
    "                continue\n",
    "            else:\n",
    "                texts.append(elem.get_text().strip())\n",
    "        return \"\\n\".join(texts)\n",
    "\n",
    "    @staticmethod\n",
    "    def path_to_uri(path: Path, scheme: str = \"https://\", domain: str = \"docs.fastht.ml\") -> str:\n",
    "        return scheme + domain + str(path).split(\"raw-data\")[-1]\n",
    "\n",
    "# src/data/preprocessor.py\n",
    "\n",
    "from typing import List, Dict\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, chunk_size: int, chunk_overlap: int):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "    def chunk_section(self, section: Dict[str, str]) -> List[Dict[str, str]]:\n",
    "        chunks = self.text_splitter.create_documents(\n",
    "            texts=[section[\"text\"]],\n",
    "            metadatas=[{\"source\": section[\"source\"]}]\n",
    "        )\n",
    "        return [{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks]\n",
    "\n",
    "# src/embedding/embedder.py\n",
    "\n",
    "from typing import List, Dict\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        )\n",
    "\n",
    "    def embed_chunks(self, chunks: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        embeddings = self.embedding_model.embed_documents(texts)\n",
    "        return [\n",
    "            {\"text\": chunk[\"text\"], \"source\": chunk[\"source\"], \"embedding\": embedding}\n",
    "            for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "# src/storage/vector_store.py\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, embedding_model, persist_directory: str):\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=\"fasthtml_docs\",\n",
    "            embedding_function=embedding_model,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "    def add_documents(self, documents: List[Dict[str, str]]):\n",
    "        docs = [\n",
    "            Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "            for doc in documents\n",
    "        ]\n",
    "        self.vector_store.add_documents(docs)\n",
    "        logger.info(f\"Added {len(docs)} documents to the vector store\")\n",
    "\n",
    "    def similarity_search(self, query: str, k: int = 5) -> List[Dict[str, str]]:\n",
    "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "        return [\n",
    "            {\n",
    "                \"text\": doc.page_content,\n",
    "                \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "                \"score\": score\n",
    "            }\n",
    "            for doc, score in results\n",
    "        ]\n",
    "\n",
    "# src/query/searcher.py\n",
    "\n",
    "from typing import List, Dict\n",
    "from src.storage.vector_store import VectorStore\n",
    "\n",
    "class Searcher:\n",
    "    def __init__(self, vector_store: VectorStore):\n",
    "        self.vector_store = vector_store\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Dict[str, str]]:\n",
    "        return self.vector_store.similarity_search(query, k)\n",
    "\n",
    "# src/utils/helpers.py\n",
    "\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "def print_resource_usage():\n",
    "    cpu_percent = psutil.cpu_percent()\n",
    "    mem_percent = psutil.virtual_memory().percent\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_util = gpus[0].load * 100 if gpus else 0\n",
    "    print(f\"CPU: {cpu_percent}%, Memory: {mem_percent}%, GPU: {gpu_util}%\")\n",
    "\n",
    "# scripts/download_docs.py\n",
    "\n",
    "from src.data.downloader import HTMLDownloader\n",
    "from config.settings import settings\n",
    "\n",
    "def main():\n",
    "    downloader = HTMLDownloader(settings.BASE_URL, settings.RAW_DATA_DIR)\n",
    "    downloader.download_all()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# scripts/process_docs.py\n",
    "\n",
    "from pathlib import Path\n",
    "from src.data.extractor import HTMLExtractor\n",
    "from src.data.preprocessor import Preprocessor\n",
    "from config.settings import settings\n",
    "\n",
    "def main():\n",
    "    extractor = HTMLExtractor()\n",
    "    preprocessor = Preprocessor(settings.CHUNK_SIZE, settings.CHUNK_OVERLAP)\n",
    "\n",
    "    raw_data_dir = Path(settings.RAW_DATA_DIR)\n",
    "    processed_data_dir = Path(settings.PROCESSED_DATA_DIR)\n",
    "    processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for html_file in raw_data_dir.glob(\"*.html\"):\n",
    "        sections = extractor.extract_sections(html_file)\n",
    "        chunks = []\n",
    "        for section in sections:\n",
    "            chunks.extend(preprocessor.chunk_section(section))\n",
    "        \n",
    "        output_file = processed_data_dir / f\"{html_file.stem}.json\"\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# scripts/create_embeddings.py\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from src.embedding.embedder import Embedder\n",
    "from src.storage.vector_store import VectorStore\n",
    "from config.settings import settings\n",
    "\n",
    "def main():\n",
    "    embedder = Embedder(settings.EMBEDDING_MODEL_NAME)\n",
    "    vector_store = VectorStore(embedder.embedding_model, settings.VECTOR_STORE_DIR)\n",
    "\n",
    "    processed_data_dir = Path(settings.PROCESSED_DATA_DIR)\n",
    "\n",
    "    for json_file in processed_data_dir.glob(\"*.json\"):\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            chunks = json.load(f)\n",
    "        \n",
    "        embedded_chunks = embedder.embed_chunks(chunks)\n",
    "        vector_store.add_documents(embedded_chunks)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# scripts/query_docs.py\n",
    "\n",
    "from src.query.searcher import Searcher\n",
    "from src.storage.vector_store import VectorStore\n",
    "from src.embedding.embedder import Embedder\n",
    "from config.settings import settings\n",
    "\n",
    "def main():\n",
    "    embedder = Embedder(settings.EMBEDDING_MODEL_NAME)\n",
    "    vector_store = VectorStore(embedder.embedding_model, settings.VECTOR_STORE_DIR)\n",
    "    searcher = Searcher(vector_store)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Enter your query (or 'quit' to exit): \")\n",
    "        if query.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        results = searcher.search(query)\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\nResult {i}:\")\n",
    "            print(f\"Score: {result['score']}\")\n",
    "            print(f\"Source: {result['source']}\")\n",
    "            print(f\"Text: {result['text'][:200]}...\")  # Display first 200 characters\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# requirements.txt\n",
    "\n",
    "requests==2.26.0\n",
    "beautifulsoup4==4.10.0\n",
    "tenacity==8.0.1\n",
    "tqdm==4.62.3\n",
    "pydantic==1.8.2\n",
    "langchain==0.0.184\n",
    "langchain-openai==0.0.2\n",
    "langchain-huggingface==0.0.3\n",
    "langchain-chroma==0.0.4\n",
    "psutil==5.8.0\n",
    "gputil==1.4.0\n",
    "\n",
    "# setup.py\n",
    "\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"fasthtml-docs-bot\",\n",
    "    version=\"0.1\",\n",
    "    packages=find_packages(),\n",
    "    install_requires=[\n",
    "        \"requests\",\n",
    "        \"beautifulsoup4\",\n",
    "        \"tenacity\",\n",
    "        \"tqdm\",\n",
    "        \"pydantic\",\n",
    "        \"langchain\",\n",
    "        \"langchain-openai\",\n",
    "        \"langchain-huggingface\",\n",
    "        \"langchain-chroma\",\n",
    "        \"psutil\",\n",
    "        \"gputil\",\n",
    "    ],\n",
    "    entry_points={\n",
    "        \"console_scripts\": [\n",
    "            \"download-docs=scripts.download_docs:main\",\n",
    "            \"process-docs=scripts.process_docs:main\",\n",
    "            \"create-embeddings=scripts.create_embeddings:main\",\n",
    "            \"query-docs=scripts.query_docs:main\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "# README.md\n",
    "\n",
    "# FastHTML Docs Bot\n",
    "\n",
    "This project is a documentation bot for FastHTML, capable of downloading HTML documents, processing them, creating embeddings, and enabling semantic search queries.\n",
    "\n",
    "## Installation\n",
    "\n",
    "1. Clone the repository:\n",
    "   ```\n",
    "   git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f8941c2-07f7-48bc-94aa-2d36f9661ead",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2463408557.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    I have made few changes to add additional functionalities. Review the files and let me if any optimization is possible. Include the changes to our existing code files and create new files using best practices.\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "I have made few changes to add additional functionalities. Review the files and let me if any optimization is possible. Include the changes to our existing code files and create new files using best practices.\n",
    "Code:\n",
    "\n",
    "```\n",
    "# extract content\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "\n",
    "from config.settings import settings\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def extract_sections(soup: BeautifulSoup, uri: str) -> List[Dict]:\n",
    "        sections = soup.find_all(\"section\")\n",
    "        section_list = []\n",
    "        for i, section in enumerate(sections):\n",
    "            section_id = section.get(\"id\")\n",
    "            section_text = extract_text_from_section(section)\n",
    "            if section_id:\n",
    "                section_data = {\n",
    "                    \"source\": f\"{uri}#{section_id}\",\n",
    "                    \"text\": section_text,\n",
    "                    \"previous_section\": section_list[i-1]['source'] if i > 0 else None,\n",
    "                    \"next_section\": None,\n",
    "                    \"metadata\": {\n",
    "                        \"page_heading\": soup.find(\"h1\").get_text().strip() if soup.find(\"h1\") else Path(uri).stem,\n",
    "                        \"section_id\": section_id\n",
    "                    }\n",
    "                }\n",
    "                if i > 0:\n",
    "                    section_list[i-1]['next_section'] = section_data['source']\n",
    "                section_list.append(section_data)\n",
    "        return section_list\n",
    "\n",
    "def extract_text_from_section(section) -> str:\n",
    "        texts = []\n",
    "        for element in section.children:\n",
    "            if isinstance(element, NavigableString):\n",
    "                if element.strip():\n",
    "                    texts.append(element.strip())\n",
    "            elif element.name != 'section':\n",
    "                texts.append(element.get_text().strip())\n",
    "        return clean_text(\" \".join(texts))\n",
    "\n",
    "def path_to_uri(path: Path, scheme: str = \"https://\", domain: str = \"docs.fastht.ml\") -> str:\n",
    "        relative_path = str(path.relative_to(settings.RAW_DATA_DIR)).replace(\"\\\\\", \"/\")\n",
    "        return scheme + domain + \"/\" + relative_path\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Replace multiple newlines with a single space\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "def process_html_files(html_files_path: List[Path]) -> List[Dict]:\n",
    "    docs_text = []\n",
    "    for record in html_files_path:\n",
    "        print(\"processing: \", record)\n",
    "        with open(record, \"r\", encoding=\"utf-8\") as html_file:\n",
    "            soup = BeautifulSoup(html_file, \"html.parser\")\n",
    "        uri = path_to_uri(path=record)\n",
    "        sections = extract_sections(soup, uri)\n",
    "        docs_text.append(sections)\n",
    "    return docs_text\n",
    "\n",
    "html_files_path = [path for path in settings.RAW_DATA_DIR.rglob(\"*.html\") if not path.is_dir()]\n",
    "docs_text = process_html_files(html_files_path)\n",
    "print(f\"Total documents processed: {len(docs_text)}\")\n",
    "\n",
    "# Preprocessing\n",
    "from typing import List, Dict\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, chunk_size: int, chunk_overlap: int):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "    def chunk_section(self, section: Dict[str, str]) -> List[Dict[str, str]]:\n",
    "        chunks = self.text_splitter.create_documents(\n",
    "            texts=[section[\"text\"]],\n",
    "            metadatas=[{\n",
    "                \"source\": section[\"source\"],\n",
    "                \"previous_section\": section.get(\"previous_section\"),\n",
    "                \"next_section\": section.get(\"next_section\"),\n",
    "                \"metadata\": section.get(\"metadata\")\n",
    "            }]\n",
    "        )\n",
    "        return [{\n",
    "            \"text\": chunk.page_content,\n",
    "            \"source\": chunk.metadata[\"source\"],\n",
    "            \"previous_section\": chunk.metadata[\"previous_section\"],\n",
    "            \"next_section\": chunk.metadata[\"next_section\"],\n",
    "            \"metadata\": chunk.metadata[\"metadata\"]\n",
    "        } for chunk in chunks]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    preprocessor = Preprocessor(chunk_size=settings.CHUNK_SIZE, chunk_overlap=settings.CHUNK_OVERLAP)\n",
    "    chunked_docs_text = []\n",
    "    for doc in docs_text:\n",
    "        for section in doc:\n",
    "            chunked_sections = preprocessor.chunk_section(section)\n",
    "            chunked_docs_text.extend(chunked_sections)\n",
    "    \n",
    "    print(f\"Total chunks created: {len(chunked_docs_text)}\")\n",
    "\n",
    "# Embedding content\n",
    "from typing import List, Dict\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# get_embedding_model: Returns an embedding model based on the specified model name.\n",
    "# OpenAIEmbeddings: Uses OpenAI’s API for embedding.\n",
    "# HuggingFaceEmbeddings: Uses Hugging Face’s models for embedding.\n",
    "\n",
    "def get_embedding_model(embedding_model_name, model_kwargs, encode_kwargs):\n",
    "    if embedding_model_name == \"text-embedding-ada-002\":\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            model = embedding_model_name,\n",
    "            openai_api_base = os.environ[\"OPENAI_API_BASE\"],\n",
    "            openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        )\n",
    "    else:\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name = embedding_model_name, # also works with model_path\n",
    "            model_kwargs = model_kwargs,\n",
    "            encode_kwargs = encode_kwargs\n",
    "        )\n",
    "    return embedding_model\n",
    "\n",
    "# EmbedChunks: A class to embed chunks using the specified model.\n",
    "# init: Initializes the embedding model.\n",
    "# call: Embeds the text in the batch and returns the embeddings along with the original text and source.\n",
    "\n",
    "class EmbedChunks:\n",
    "    def __init__(self, model_name):\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            embedding_model_name = model_name,\n",
    "            model_kwargs = {\"device\": \"cuda\"},\n",
    "            encode_kwargs = {\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        embeddings = self.embedding_model.embed_documents(batch[\"text\"])\n",
    "        return {\"text\": batch[\"text\"], \"source\": batch[\"source\"], \"embeddings\": embeddings}\n",
    "\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        )\n",
    "\n",
    "    def embed_chunks(self, chunks: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        embeddings = self.embedding_model.embed_documents(texts)\n",
    "        return [\n",
    "            {\"text\": chunk[\"text\"], \"source\": chunk[\"source\"], \"embedding\": embedding}\n",
    "            for chunk, embedding in zip(chunks, embeddings)\n",
    "        ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    embedder = Embedder(model_name=settings.EMBEDDING_MODEL_NAME)\n",
    "    embedded_chunks = embedder.embed_chunks(chunked_docs_text)\n",
    "\n",
    "    print(f\"Total embeddings created: {len(embedded_chunks)}\")\n",
    "\n",
    "# store vectors\n",
    "# src/storage/vector_store.py\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, embedding_model, persist_directory: Path):\n",
    "        self.vector_store = Chroma(\n",
    "            collection_name=\"fasthtml_docs_db\",\n",
    "            embedding_function=embedding_model,\n",
    "            persist_directory=os.path.abspath(persist_directory)\n",
    "        )\n",
    "\n",
    "    def add_documents(self, documents: List[Dict[str, str]]):\n",
    "        docs = [\n",
    "            Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "            for doc in documents\n",
    "        ]\n",
    "        self.vector_store.add_documents(docs)\n",
    "        logger.info(f\"Added {len(docs)} documents to the vector store\")\n",
    "\n",
    "    def similarity_search(self, query: str, k: int = 5) -> List[Dict[str, str]]:\n",
    "        results = self.vector_store.similarity_search_with_score(query, k=k)\n",
    "        return [\n",
    "            {\n",
    "                \"text\": doc.page_content,\n",
    "                \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "                \"score\": score\n",
    "            }\n",
    "            for doc, score in results\n",
    "        ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=settings.EMBEDDING_MODEL_NAME,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100}\n",
    "    )\n",
    "\n",
    "    # Initialize the vector store\n",
    "    vectorStore = VectorStore(embedding_model=embedding_model, persist_directory=settings.VECTOR_STORE_DIR)\n",
    "    \n",
    "    # Assuming embedded_chunks is already defined and contains the embedded documents\n",
    "    vectorStore.add_documents(embedded_chunks)\n",
    "\n",
    "    # Assuming you have a client instance to interact with the Chroma DB\n",
    "    client = vectorStore.vector_store._client\n",
    "    collection = client.get_collection(name=\"fasthtml_docs_db\")\n",
    "\n",
    "    print(f\"Total items in {collection}: {collection.count()}\")\n",
    "\n",
    "# Generate Response\n",
    "import openai\n",
    "import time\n",
    "\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "def response_stream(chat_completion, llm):\n",
    "    if llm.startswith(\"gpt\"):\n",
    "        for chunk in chat_completion:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content is not None:\n",
    "                yield content\n",
    "    else:\n",
    "        for chunk in chat_completion:\n",
    "            yield chunk\n",
    "\n",
    "def prepare_response(chat_completion, stream, llm):\n",
    "    if stream:\n",
    "        return response_stream(chat_completion, llm)\n",
    "    else:\n",
    "        if llm.startswith(\"gpt\"):\n",
    "            return chat_completion.choices[0].message.content\n",
    "        else:\n",
    "            return chat_completion\n",
    "\n",
    "\n",
    "def get_client(llm):\n",
    "    if llm.startswith(\"gpt\"):\n",
    "        base_url = os.environ[\"OPENAI_API_BASE\"]\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "        client = openai.OpenAI(base_url=base_url, api_key=api_key)\n",
    "    else:\n",
    "        #base_url = os.environ[\"HUGGINGFACEHUB_API_BASE\"]\n",
    "        api_key = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "        client = HuggingFaceEndpoint(huggingfacehub_api_token=api_key, repo_id=llm)\n",
    "    \n",
    "    return client\n",
    "\n",
    "def generate_response(\n",
    "    llm, temperature=0.0, stream=True, \n",
    "    system_content=\"\", assistant_content=\"\", user_content=\"\", \n",
    "    max_retries=1, retry_interval=60):\n",
    "    \"\"\"Generate response from an LLM\"\"\"\n",
    "    retry_count = 0\n",
    "    client = get_client(llm=llm)\n",
    "    \n",
    "    prompt = [(\"system\", system_content), (\"assistant\", assistant_content), (\"user\", user_content)]\n",
    "    messages = [{\"role\": role, \"content\": content} for role, content in prompt if content]\n",
    "\n",
    "    while retry_count <= max_retries:\n",
    "        try:\n",
    "            if llm.startswith(\"gpt\"):\n",
    "                chat_completion = client.chat.completions.create(\n",
    "                    model=llm,\n",
    "                    temperature=temperature,\n",
    "                    stream=stream,\n",
    "                    messages=messages,\n",
    "                )\n",
    "            else:\n",
    "                chat_completion = client.invoke(\n",
    "                    repo_id=llm,\n",
    "                    temperature=temperature,\n",
    "                    streaming=stream,\n",
    "                    input=messages,\n",
    "                )\n",
    "            return prepare_response(chat_completion, stream=stream, llm=llm)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Exception: {e}')\n",
    "            time.sleep(retry_interval) # default is pre-minute rate limits\n",
    "            retry_count += 1\n",
    "    return \"\"    \n",
    "\n",
    "context_results = vectorStore.similarity_search('how to install fasthtml?')\n",
    "context = [item[\"text\"] for item in context_results]\n",
    "print(context)\n",
    "\n",
    "# Generate response\n",
    "query = 'how to install fasthtml?'\n",
    "response = generate_response(\n",
    "    llm=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    temperature=0.1,\n",
    "    stream=True,\n",
    "    system_content=\"Answer the query using the context provided. Be succinct.\",\n",
    "    user_content=f\"query: {query}, context: {context}\"\n",
    ")\n",
    "\n",
    "# Stream response\n",
    "for content in response:\n",
    "    print(content, end='', flush=True)\n",
    "\n",
    "# Create Query agent\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "def get_num_tokens(text):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))\n",
    "\n",
    "\n",
    "def trim(text, max_context_length):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return enc.decode(enc.encode(text)[:max_context_length])\n",
    "\n",
    "class QueryAgent:\n",
    "    def __init__(self, embedding_model_name=\"thenlper/gte-base\",\n",
    "                 llm=\"mistralai/Mistral-7B-Instruct-v0.3\", temperature=0.1, \n",
    "                 max_context_length=4096, system_content=\"\", assistant_content=\"\"\n",
    "                ):\n",
    "        # Embedding model\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            embedding_model_name = embedding_model_name,\n",
    "            model_kwargs = {\"device\": \"cuda\"},\n",
    "            encode_kwargs = {\"device\": \"cuda\", \"batch_size\": 100}\n",
    "        )\n",
    "\n",
    "        # Context length (restrict input length to 50% of total context length)\n",
    "        max_context_length = int(0.5*max_context_length)\n",
    "\n",
    "        # LLM\n",
    "        self.llm = llm\n",
    "        self.temperature = temperature\n",
    "        self.context_length = max_context_length - get_num_tokens(system_content + assistant_content)\n",
    "        self.system_content = system_content\n",
    "        self.assistant_content = assistant_content\n",
    "\n",
    "    def __call__(self, query, num_chunks=5, stream=True):\n",
    "        # Get sources and context\n",
    "        # Initialize the vector store\n",
    "# VectorStore is class defined above for storing vectors. use proper import to access this class\n",
    "        vectorStore = VectorStore(embedding_model=embedding_model_name, persist_directory=settings.VECTOR_STORE_DIR)\n",
    "        context_results = vectorStore.similarity_search(query=query)\n",
    "\n",
    "        # Generate response\n",
    "        context = [item[\"text\"] for item in context_results]\n",
    "        sources = [item[\"source\"] for item in context_results]\n",
    "        user_content = f\"query: {query}, context: {context}\"\n",
    "        answer = generate_response(\n",
    "            llm=self.llm,\n",
    "            temperature=self.temperature,\n",
    "            stream=stream,\n",
    "            system_content=self.system_content,\n",
    "            assistant_content=self.assistant_content,\n",
    "            user_content=trim(user_content, self.context_length))\n",
    "\n",
    "        # Result\n",
    "        result = {\n",
    "            \"question\": query,\n",
    "            \"sources\": sources,\n",
    "            \"answer\": answer,\n",
    "            \"llm\": self.llm,\n",
    "        }\n",
    "        return result\n",
    "\n",
    "import json\n",
    "from config.config import MAX_CONTEXT_LENGTHS\n",
    "\n",
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "llm = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "query = 'how to install fasthtml?'\n",
    "system_content = \"Answer the query using the context provided. Be succinct.\"\n",
    "agent = QueryAgent(\n",
    "    embedding_model_name=embedding_model_name,\n",
    "    llm=llm,\n",
    "    max_context_length=MAX_CONTEXT_LENGTHS[llm],\n",
    "    system_content=system_content)\n",
    "result = agent(query=query, stream=False)\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "\n",
    "# config/config.py\n",
    "\n",
    "# Embedding dimensions\n",
    "EMBEDDING_DIMENSIONS = {\n",
    "    \"thenlper/gte-base\": 768,\n",
    "    \"thenlper/gte-large\": 1024,\n",
    "    \"BAAI/bge-large-en\": 1024,\n",
    "    \"text-embedding-ada-002\": 1536,\n",
    "    \"gte-large-fine-tuned\": 1024,\n",
    "}\n",
    "\n",
    "# Maximum context lengths\n",
    "MAX_CONTEXT_LENGTHS = {\n",
    "    \"gpt-4\": 8192,\n",
    "    \"gpt-3.5-turbo\": 4096,\n",
    "    \"gpt-3.5-turbo-16k\": 16384,\n",
    "    \"gpt-4-1106-preview\": 128000,\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\": 4096,\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\": 4096,\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\": 4096,\n",
    "    \"meta-llama/Llama-3-8b-chat-hf\": 8192,\n",
    "    \"meta-llama/Llama-3-70b-chat-hf\": 8192,\n",
    "    \"codellama/CodeLlama-34b-Instruct-hf\": 16384,\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\": 65536,\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\": 32768,\n",
    "    \"mistralai/Mixtral-8x22B-Instruct-v0.1\": 65536,\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\": 32768\n",
    "}\n",
    "\n",
    "# Pricing per 1M tokens\n",
    "PRICING = {\n",
    "    \"gpt-3.5-turbo\": {\"prompt\": 1.5, \"sampled\": 2},\n",
    "    \"gpt-4\": {\"prompt\": 30, \"sampled\": 60},\n",
    "    \"gpt-4-1106-preview\": {\"prompt\": 10, \"sampled\": 30},\n",
    "    \"llama-2-7b-chat-hf\": {\"prompt\": 0.15, \"sampled\": 0.15},\n",
    "    \"llama-2-13b-chat-hf\": {\"prompt\": 0.25, \"sampled\": 0.25},\n",
    "    \"llama-2-70b-chat-hf\": {\"prompt\": 1, \"sampled\": 1},\n",
    "    \"llama-3-8b-chat-hf\": {\"prompt\": 0.15, \"sampled\": 0.15},\n",
    "    \"llama-3-70b-chat-hf\": {\"prompt\": 1, \"sampled\": 1},\n",
    "    \"codellama-34b-instruct-hf\": {\"prompt\": 1, \"sampled\": 1},\n",
    "    \"mistral-7b-instruct-v0.1\": {\"prompt\": 0.15, \"sampled\": 0.15},\n",
    "    \"mixtral-8x7b-instruct-v0.1\": {\"prompt\": 0.50, \"sampled\": 0.50},\n",
    "    \"mixtral-8x22b-instruct-v0.1\": {\"prompt\": 0.9, \"sampled\": 0.9},\n",
    "}\n",
    "\n",
    "\n",
    "# config/settings.py\n",
    "\n",
    "from pathlib import Path\n",
    "from pydantic_settings import BaseSettings\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    BASE_DIR: Path = Path(__file__).resolve().parent.parent\n",
    "    BASE_URL: str = \"https://docs.fastht.ml/\"\n",
    "    RAW_DATA_DIR: Path = BASE_DIR / \"data\" / \"raw-data\"\n",
    "    PROCESSED_DATA_DIR: Path = BASE_DIR / \"data\" / \"processed-data\"\n",
    "    VECTOR_STORE_DIR: Path = BASE_DIR / \"data\" / \"docs_db\"\n",
    "    CHUNK_SIZE: int = 500\n",
    "    CHUNK_OVERLAP: int = 50\n",
    "    EMBEDDING_MODEL_NAME: str = \"thenlper/gte-base\"\n",
    "\n",
    "    class Config:\n",
    "        env_file = \".env\"\n",
    "        extra = \"allow\"  # Allow extra fields\n",
    "\n",
    "settings = Settings()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602aa51-c366-4f85-99a3-89ca85e6e800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
